{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying On-Disk Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * Compare queries of tabular data for **on-disk** containers\n",
    "> * Compare sizes and times for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipython_memwatcher import MemWatcher\n",
    "mw = MemWatcher()\n",
    "mw.start_watching_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movielens datasets in pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start loadind dataset in pandas so that we can create persistent versions more easily later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dset = 'movielens-1m'\n",
    "fdata = os.path.join(dset, 'ratings.dat.gz')\n",
    "fitem = os.path.join(dset, 'movies.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import CSV files via pandas\n",
    "import pandas as pd\n",
    "# pass in column names for each CSV\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(fdata, sep=';', names=r_cols, compression='gzip')\n",
    "\n",
    "m_cols = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(fitem, sep=';', names=m_cols,\n",
    "                     dtype={'title': object, 'genres': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store movies and ratings in 2 separate tables in SQLite\n",
    "sqlite_norm = \"movielens-norm.sqlite\"\n",
    "if os.path.exists(sqlite_norm):\n",
    "    os.unlink(sqlite_norm)\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(sqlite_norm)\n",
    "conn.text_factory = str   # Shut up problems with Unicode\n",
    "ratings.to_sql(\"ratings\", conn)\n",
    "movies.to_sql(\"movies\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create one merged DataFrame\n",
    "lens = pd.merge(movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lens.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store lens denormalized in 1 table in SQLite\n",
    "sqlite_denorm = \"movielens-denorm.sqlite\"\n",
    "if os.path.exists(sqlite_denorm):\n",
    "    os.unlink(sqlite_denorm)\n",
    "conn_denorm = sqlite3.connect(sqlite_denorm)\n",
    "conn_denorm.text_factory = str   # Shut up problems with Unicode\n",
    "lens.to_sql(\"lens\", conn_denorm)\n",
    "conn_denorm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get rid of unnecessary containers to save memory\n",
    "del movies, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = lens.query(\"(title == 'Tom and Huck (1995)') & (rating == 5)\")['user_id']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pandas_mem = mw.measurements.time_delta\n",
    "qtime_pandas_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bcolz` cannot only deal with data in-memory, but also on-disk using exactly the same API.  Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "bcolz.print_versions()\n",
    "bcolz.defaults.cparams['cname'] = 'lz4hc'\n",
    "bcolz.defaults.cparams['clevel'] = 6\n",
    "bcolz.defaults.cparams['shuffle'] = bcolz.BITSHUFFLE\n",
    "bcolz.set_nthreads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import a pandas DataFrame in a ctable on-disk\n",
    "bcolz_dir = \"movielens-denorm.bcolz\"\n",
    "if os.path.exists(bcolz_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(bcolz_dir)\n",
    "zlens = bcolz.ctable.fromdataframe(lens, rootdir=bcolz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Workaround to use bytes instead of Unicode (not supported yet in numexpr queries)\n",
    "title = zlens['title'][:].astype('S82')\n",
    "zlens.delcol('title')\n",
    "zlens.addcol(title, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Workaround to use bytes instead of Unicode (not supported yet in numexpr queries)\n",
    "genres = zlens['genres'][:].astype('S47')\n",
    "zlens.delcol('genres')\n",
    "zlens.addcol(genres, 'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = zlens[\"(title == b'Tom and Huck (1995)') & (rating == 5)\"]['user_id']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_bcolz = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One can optimize the query more\n",
    "%time result = [r.user_id for r in zlens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\", outcols=['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_bcolz_opt = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the space consumed on-disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh $bcolz_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's compare it with the internal estimated size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the internal estimated size is a bit higher than the actual size on-disk, which means that bcolz offers a conservative guess in general.\n",
    "\n",
    "But there are other ways to store compressed tables.  Let's visit PyTables and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5denorm = \"movielens-denorm.h5\"\n",
    "if os.path.exists(h5denorm):\n",
    "    os.unlink(h5denorm)\n",
    "zlens.tohdf5(h5denorm, nodepath='/h5lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "h5file = tables.open_file(h5denorm)\n",
    "h5lens = h5file.root.h5lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the HDF5 table\n",
    "h5lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, it seems that PyTables is ~2x slower than bcolz.  What about the size of the HDF5 file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5file.close()\n",
    "!ls -lh $h5denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.0 MB is approximately the same than the 6.0 MB that used bcolz (which is expected because both are using LZ4HC as the compressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (PyTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables can index tables for improving query speed.  Let's try that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Copy the original PyTables table into another file\n",
    "import shutil\n",
    "h5idx = \"movielens-indexed.h5\"\n",
    "if os.path.exists(h5idx):\n",
    "    os.unlink(h5idx)\n",
    "shutil.copyfile(h5denorm, h5idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the new file in 'a'ppend mode\n",
    "h5i = tables.open_file(h5idx, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an index for the 'title' column\n",
    "h5lens = h5i.root.h5lens\n",
    "h5lens.cols.title.create_csindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...and redo the query...\n",
    "t = %timeit -r1 -n1 -o result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables_index1 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by indexing one column we have got more than 20x of acceleration wrt PyTables.  What happens if we index the 'rating' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5lens.cols.rating.create_csindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = %timeit -r1 -n1 -o result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables_index2 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's another ~3x or ~4x additional acceleration, and the best time that we ever reached for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5i.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying on-disk data with SQLite (relational database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute the query with the de-normalized SQLite database\n",
    "conn = sqlite3.connect(sqlite_denorm)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "%time result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_denorm = mw.measurements.time_delta\n",
    "# ...and print the result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute the query with the normalized SQLite database\n",
    "conn = sqlite3.connect(sqlite_norm)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "select ratings.user_id from movies \n",
    "INNER JOIN ratings ON movies.movie_id = ratings.movie_id\n",
    "where movies.title == 'Tom and Huck (1995)' and ratings.rating == 5\n",
    "\"\"\"\n",
    "%time result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_norm = mw.measurements.time_delta\n",
    "# ...and print the result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in general, we see that it is much faster to query tables in denormalized form, although they take much more storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -lh $sqlite_norm $sqlite_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some way, storing tables in normalized form is a kind of compression, but that comes to the cost of using more time to process queries.  But indexing is the strongest point of relational databases, so let's see how much it can accelerate queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (SQLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_idx = \"movielens-indexed.sqlite\"\n",
    "if os.path.exists(sqlite_idx):\n",
    "    os.unlink(sqlite_idx)\n",
    "shutil.copyfile(sqlite_denorm, sqlite_idx)\n",
    "conn_idx = sqlite3.connect(sqlite_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = conn_idx.cursor()\n",
    "c.execute(\"CREATE INDEX index_title ON lens (title)\")\n",
    "conn_idx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "t = %timeit -r1 -n1 -o result = [r for r in c.execute(q)]\n",
    "# keep the run-time for reference\n",
    "qtime_sqlite_index1 = t.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, with quite less than 1 ms we are getting the best figure so far, and faster than the best figure with indexed PyTables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "What will happen if we index the rating column?  We should get better speed, right?  Try it out and report the improvement that you are seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query times\n",
    "labels = [\"pandas (in-memory)\", \"bcolz\", \"PyTables\", \"SQLite denorm\", \"SQLite norm\", \"PyTables (indexed)\", \"SQLite (indexed)\"]\n",
    "df = pd.DataFrame({'time (sec)': [qtime_pandas_mem, qtime_bcolz_opt, qtime_pytables, qtime_sqlite_denorm, qtime_sqlite_norm, qtime_pytables_index2, qtime_sqlite_index1]}, index=labels)\n",
    "df.plot(kind='barh', figsize=(12,5), fontsize=16, title=\"Query times for MovieLens 1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh movielens* | sort -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Container sizes\n",
    "df = pd.DataFrame({'size (MB)': [160, 6.0, 6.0, 78, 37, 9.9, 119]}, index=labels)\n",
    "df.plot(kind='barh', figsize=(12,5), fontsize=16, title=\"Container sizes for MovieLens 1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are many different solutions for storing large datasets both in-memory and on-disk.  Here it is a summary of the ones that we have seen in this tutorial:\n",
    "\n",
    "* In-core\n",
    "  * Python lists and dictionaries: Included in Python.  Very flexible.  Not efficient for large datasets.\n",
    "  * NumPy: Good for large amounts of data and with a lot of functionality.  Does not support compression.\n",
    "  * Pandas: Good for large amounts of data and with a lot of functionality.  Does not support compression.\n",
    "  * bcolz: Good for large amounts of data, but reduced functionality.  Supports compression.\n",
    "  \n",
    "* Out-of-core\n",
    "  * bcolz: Good for large datasets.  Supports compression, but not indexing.  Medium query speeds.\n",
    "  * SQLite: Can be used for large datasets, but requires lots of storage.  Supports indexing.  Excellent query speeds.\n",
    "  * PyTables: Good for large datasets.  Supports indexing and compression.  Good query speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, every solution has strengths and weaknesses, with wild variations in resource consumptions, so a wise thing to do is to know them better and try to apply the best candidate to your scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note, this series of tutorials is work in progress, so please contact the author in case you see some inacuracy or just want to send feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hope you enjoyed the ride!**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
